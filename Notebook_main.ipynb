{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import xmltodict\n",
        "import logging\n",
        "\n",
        "load_dotenv()  # This loads the environment variables from .env\n",
        "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
        "aws_secret_access_key = os.environ.get(\n",
        "    'AWS_SECRET_ACCESS_KEY')\n",
        "region_name = 'us-east-1'\n",
        "\n",
        "#endpoint_url = 'https://mturk-requester-sandbox.us-east-1.amazonaws.com'\n",
        "\n",
        "# Uncomment this line to use in production\n",
        "endpoint_url = 'https://mturk-requester.us-east-1.amazonaws.com'\n",
        "\n",
        "mturk = boto3.client(\n",
        "    'mturk',\n",
        "    endpoint_url=endpoint_url,\n",
        "    region_name=region_name,\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,\n",
        ")\n",
        "\n",
        "# This will return $10,000.00 in the MTurk Developer Sandbox\n",
        "print(mturk.get_account_balance()['AvailableBalance'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing - xml - hits - qualifications "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XML production helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import os\n",
        "import chardet  # You might need to install this package\n",
        "import re  # Import regex module\n",
        "\n",
        "def remove_emojis(text):\n",
        "    # Regex to filter out emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def extract_body_from_text(file_path):\n",
        "    with open(file_path, 'rb') as file:  # Open the file in binary mode\n",
        "        raw_data = file.read()\n",
        "    try:\n",
        "        content = raw_data.decode('utf-8')  # Try to decode using UTF-8\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Unicode Decode Error in file: {file_path}. Skipping file.\")\n",
        "        return None\n",
        "\n",
        "    if 'Body:' in content and 'Category:' in content:\n",
        "        body = content.split('Body:')[1].split('Category:')[0].strip()\n",
        "        body = remove_emojis(body)\n",
        "        return body\n",
        "    else:\n",
        "        print(f\"Markers not found in file: {file_path}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_qa_pairs(content):\n",
        "    qa_pairs = []\n",
        "    true_qa_section = content.split('True QA:')[1].split('False QA:')[0].strip()\n",
        "    false_qa_sections = content.split('False QA:')[1:]\n",
        "\n",
        "    # Extract True QA pairs\n",
        "    true_qa_parts = [part for part in true_qa_section.split('\\n') if part.startswith('Question') or part.startswith('Answer')]\n",
        "    for i in range(0, len(true_qa_parts), 2):\n",
        "        if i+1 < len(true_qa_parts):\n",
        "            question = true_qa_parts[i].split('Question')[1].strip()\n",
        "            answer = true_qa_parts[i+1].split('Answer')[1].strip()\n",
        "            qa_pairs.append((question, answer, True))  # True indicates it's a correct pair\n",
        "\n",
        "    # Extract False QA pairs\n",
        "    for section in false_qa_sections:\n",
        "        parts = [part for part in section.split('\\n') if part.startswith('Question') or part.startswith('Answer')]\n",
        "        for i in range(0, len(parts), 2):\n",
        "            if i+1 < len(parts):\n",
        "                question = parts[i].split('Question')[1].strip()\n",
        "                answer = parts[i+1].split('Answer')[1].strip()\n",
        "                qa_pairs.append((question, answer, False))  # False indicates it's an incorrect pair\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "\n",
        "def parse_question_xml(question_xml):\n",
        "    # Parse the XML string\n",
        "    question_data = xmltodict.parse(question_xml)\n",
        "\n",
        "    # Navigate through the dictionary to extract needed information\n",
        "    # Modify the path according to your XML structure\n",
        "    questions = question_data['QuestionForm']['Overview']\n",
        "\n",
        "    parsed_questions = []\n",
        "    for question in questions:\n",
        "        if 'Title' in question:\n",
        "            title = question['Title']\n",
        "        else:\n",
        "            title = 'No Title'\n",
        "        \n",
        "        text = question['Text']\n",
        "        parsed_questions.append({'Title': title, 'Text': text})\n",
        "\n",
        "    return parsed_questions\n",
        "\n",
        "def parse_answer_xml(answer_xml):\n",
        "    parsed_data = xmltodict.parse(answer_xml)\n",
        "    parsed_answers = []\n",
        "    passed = 3\n",
        "\n",
        "    if 'QuestionFormAnswers' in parsed_data and 'Answer' in parsed_data['QuestionFormAnswers']:\n",
        "        answers = parsed_data['QuestionFormAnswers']['Answer']\n",
        "        if not isinstance(answers, list):\n",
        "            answers = [answers]\n",
        "\n",
        "        for answer in answers:\n",
        "            qid = answer['QuestionIdentifier']\n",
        "            if 'FreeText' in answer:\n",
        "                answer_text = answer['FreeText']\n",
        "            elif 'SelectionIdentifier' in answer:\n",
        "                answer_text = answer['SelectionIdentifier']\n",
        "                if \"incorrect\" in answer_text:\n",
        "                    passed -= 1\n",
        "            else:\n",
        "                answer_text = \"No Answer Text\"\n",
        "            \n",
        "            parsed_answers.append({'Question ID': qid, 'Answer': answer_text})\n",
        "\n",
        "    return parsed_answers, passed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HIT XML creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def create_HIT_test_xml(files, questions_per_text=3):\n",
        "    xml_parts = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>']\n",
        "    xml_parts.append('<QuestionForm xmlns=\"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/QuestionForm.xsd\">')\n",
        "    xml_parts.append('<Overview><Text>Velkommen til oppgaven! Vennligst les teksten nedenfor nøye. Din oppgave er å velge det korrekte spørsmål-svar paret som tilsvarer teksten og gi en oppsummering på to til tre setninger av den delen i teksten du syntes var mest interessant. Oppsumeringen skal ikke være på jeg form, og ikke inneholde annen tekst enn bare oppsummeringen. I kilder feltet skal du kopiere inn den delen av nyhetsartikkelen du oppsumerte, har du eksempelvis oppsumert fra de første tre setningene i nyhetsartikkelen skal du kopiere inn disse. Her er det viktig at du ikke skriver inn setninger som ikke finnes i den originale teksten.</Text></Overview>')\n",
        "    xml_parts.append('<Overview><Text>Teksten din vil bli evaluert basert på nøyaktigheten av svarene dine. For å unngå avvisning, er det viktig at du svarer korrekt på kontrollspørsmålene og velger et spørsmål-svar par som nøyaktig reflekterer informasjonen i teksten. Ukorrekte eller irrelevante svar kan føre til at bidraget ditt blir avvist.</Text></Overview>')\n",
        "    xml_parts.append('<Overview><Text>Du kan kvalifisere deg for en bonus basert på kvaliteten og kvantiteten av arbeidet ditt. Høykvalitetsbidrag som viser en grundig forståelse av teksten og et presist valg av spørsmål-svar par, vil øke sjansene dine for å motta en bonus. Jo flere oppgaver du fullfører med høy kvalitet, desto større er sjansen for bonus.</Text></Overview>')\n",
        "\n",
        "    for idx, file_path in enumerate(files, start=1):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        body = extract_body_from_text(file_path)\n",
        "        if not body:\n",
        "            continue\n",
        "\n",
        "        qa_pairs = extract_qa_pairs(content)\n",
        "        xml_parts.append(f'<Overview><Title>Tekst {idx}</Title><Text><![CDATA[{body}]]></Text></Overview>')\n",
        "\n",
        "        # Start grouping question-answer pairs\n",
        "        question_element = f'<Question><QuestionIdentifier>text{idx}_questions</QuestionIdentifier><IsRequired>true</IsRequired>'\n",
        "        question_element += '<QuestionContent><Text>Velg korrekt spørsmål-svar par som tilhører teksten:</Text></QuestionContent>'\n",
        "        selections = \"\"\n",
        "        for q_idx, (question, answer, is_correct) in enumerate(qa_pairs[:questions_per_text], start=1):\n",
        "            selection_id = f'{\"correct\" if is_correct else \"incorrect\"}_{idx}_{q_idx}'\n",
        "            selection = f'<Selection><SelectionIdentifier>{selection_id}</SelectionIdentifier><Text><![CDATA[Spørsmål {question} Svar {answer}]]></Text></Selection>'\n",
        "            selections += selection\n",
        "\n",
        "        question_element += f'<AnswerSpecification><SelectionAnswer><StyleSuggestion>radiobutton</StyleSuggestion><Selections>{selections}</Selections></SelectionAnswer></AnswerSpecification></Question>'\n",
        "        xml_parts.append(question_element)\n",
        "\n",
        "        # Add the summary question with constraints\n",
        "        summary_question = f'<Question><QuestionIdentifier>text{idx}_summary</QuestionIdentifier><IsRequired>true</IsRequired>'\n",
        "        summary_question += '<QuestionContent><Text>Skriv en oppsummering på to til tre setninger av den delen i teksten du syntes var mest interessant. Oppsumeringen skal ikke være på jeg form:</Text></QuestionContent>'\n",
        "        summary_question += '<AnswerSpecification><FreeTextAnswer><Constraints><Length minLength=\"1\" /></Constraints></FreeTextAnswer></AnswerSpecification></Question>'\n",
        "        \n",
        "        xml_parts.append(summary_question)\n",
        "\n",
        "        # Add the source citation question\n",
        "        source_question = f'<Question><QuestionIdentifier>text{idx}_source</QuestionIdentifier><IsRequired>true</IsRequired>'\n",
        "        source_question += '<QuestionContent><Text>Kilde - Kopier inn den delen av nyhetsartikkelen du oppsumerte fra:</Text></QuestionContent>'\n",
        "        source_question += '<AnswerSpecification><FreeTextAnswer><Constraints><Length minLength=\"1\" /></Constraints></FreeTextAnswer></AnswerSpecification></Question>'\n",
        "        xml_parts.append(source_question)\n",
        "\n",
        "    xml_parts.append('</QuestionForm>')\n",
        "    return ''.join(xml_parts)\n",
        "\n",
        "def process_directory_in_chunks(directory, output_dir, chunk_size=3, questions_per_text=3):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    all_filenames = [os.path.join(directory, f) for f in sorted(os.listdir(directory)) if f.endswith('.txt')]\n",
        "    total_files = len(all_filenames)\n",
        "\n",
        "    for i in range(0, total_files, chunk_size):\n",
        "        chunk_files = all_filenames[i:i + chunk_size]\n",
        "\n",
        "        # Check if this is the last chunk and if it has fewer files than the chunk size\n",
        "        if i + chunk_size >= total_files and len(chunk_files) < chunk_size:\n",
        "            remaining = chunk_size - len(chunk_files)\n",
        "            chunk_files.extend(all_filenames[:remaining])\n",
        "\n",
        "        hit_xml = create_HIT_test_xml(chunk_files, questions_per_text=questions_per_text)\n",
        "\n",
        "        output_file_path = os.path.join(output_dir, f'hit_{i // chunk_size}.xml')\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(hit_xml)\n",
        "        print(f\"Created HIT XML file: {output_file_path}\")\n",
        "\n",
        "# Example Usage\n",
        "input_directory = '../new_tasks 2/'\n",
        "output_directory = './HITS5'\n",
        "process_directory_in_chunks(input_directory, output_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qualification creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Personalia qualification xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def create_question_xml(field_dict, free_text_fields):\n",
        "    question_form = ET.Element(\"QuestionForm\", xmlns=\"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/QuestionForm.xsd\")\n",
        "    \n",
        "    for field, options in field_dict.items():\n",
        "        question = ET.SubElement(question_form, \"Question\")\n",
        "        ET.SubElement(question, \"QuestionIdentifier\").text = field\n",
        "        ET.SubElement(question, \"IsRequired\").text = \"true\"\n",
        "        question_content = ET.SubElement(question, \"QuestionContent\")\n",
        "        ET.SubElement(question_content, \"Text\").text = f\"Hva er din/ditt/dine {field}?\"\n",
        "\n",
        "        if field in free_text_fields:\n",
        "            answer_spec = ET.SubElement(question, \"AnswerSpecification\")\n",
        "            free_text_answer = ET.SubElement(answer_spec, \"FreeTextAnswer\")\n",
        "            ET.SubElement(free_text_answer, \"NumberOfLinesSuggestion\").text = \"1\"\n",
        "        else:\n",
        "            answer_spec = ET.SubElement(question, \"AnswerSpecification\")\n",
        "            selection_answer = ET.SubElement(answer_spec, \"SelectionAnswer\")\n",
        "            ET.SubElement(selection_answer, \"StyleSuggestion\").text = \"radiobutton\"\n",
        "            selections = ET.SubElement(selection_answer, \"Selections\")\n",
        "\n",
        "            for option in options:\n",
        "                selection = ET.SubElement(selections, \"Selection\")\n",
        "                ET.SubElement(selection, \"SelectionIdentifier\").text = option\n",
        "                ET.SubElement(selection, \"Text\").text = option\n",
        "\n",
        "    return ET.tostring(question_form, encoding='unicode')\n",
        "\n",
        "def create_answer_key_xml(field_dict):\n",
        "    answer_key = ET.Element(\"AnswerKey\", xmlns=\"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/AnswerKey.xsd\")\n",
        "    for field in field_dict:\n",
        "        question = ET.SubElement(answer_key, \"Question\")\n",
        "        ET.SubElement(question, \"QuestionIdentifier\").text = field\n",
        "        answer_option = ET.SubElement(question, \"AnswerOption\")\n",
        "        ET.SubElement(answer_option, \"SelectionIdentifier\").text = \"anyResponse\"\n",
        "        ET.SubElement(answer_option, \"AnswerScore\").text = \"1\"\n",
        "    return ET.tostring(answer_key, encoding='unicode')\n",
        "\n",
        "\n",
        "# Example usage\n",
        "feltalternativer = {\n",
        "    \"alder\": [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"],\n",
        "    \"kjønn\": [\"Mann\", \"Kvinne\", \"Ikke-binær\", \"Foretrekker å ikke si\"],\n",
        "    \"yrke\": [\"Student\", \"Lærer\", \"Ingeniør\", \"Kunstner\", \"Annet\"],\n",
        "    \"arbeidsstatus\": [\"Heltid\", \"Deltid\", \"Arbeidsledig\", \"Pensjonert\", \"Annet\"],\n",
        "    \"interesser\": [\"Sport\", \"Teknologi\", \"Kunst\", \"Vitenskap\", \"Reise\", \"Annet\"],\n",
        "    \"hobbyer\": [\"Lesing\", \"Spilling\", \"Matlaging\", \"Fjellturer\", \"Håndarbeid\", \"Annet\"],\n",
        "    \"nyhetslesevaner\": [\"Daglig\", \"Ukentlig\", \"Månedlig\", \"Sjelden\", \"Aldri\"],\n",
        "    \"foretrukne nyhetskategorier\": [\"Politikk\", \"Økonomi\", \"Underholdning\", \"Sport\", \"Teknologi\", \"Annet\"],\n",
        "    \"Norsk språkferdigheter\": [\"Flytende\", \"Mellomnivå\", \"Grunnleggende\", \"Ingen\"],\n",
        "    # Legg til flere felt og alternativer etter behov\n",
        "}\n",
        "\n",
        "def create_qualification_type(mturk_client, name, description, test, answer_key, duration):\n",
        "    response = mturk_client.create_qualification_type(\n",
        "        Name=name,\n",
        "        Description=description,\n",
        "        Test=test,\n",
        "        AnswerKey=answer_key,\n",
        "        TestDurationInSeconds=duration,\n",
        "        QualificationTypeStatus='Active',\n",
        "        AutoGranted=False\n",
        "    )\n",
        "    return response['QualificationType']['QualificationTypeId']\n",
        "\n",
        "\n",
        "free_text_fields = []\n",
        "personalia_test_xml = create_question_xml(feltalternativer, free_text_fields)\n",
        "# Generate the answer_key_xml as needed\n",
        "print(personalia_test_xml)\n",
        "answer_key_xml = create_answer_key_xml(feltalternativer)\n",
        "print(answer_key_xml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Personalia qualification creation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "def create_qualification_type(mturk_client, name, description, test, duration):\n",
        "    response = mturk_client.create_qualification_type(\n",
        "        Name=name,\n",
        "        Description=description,\n",
        "        Test=test,\n",
        "        TestDurationInSeconds=duration,\n",
        "        QualificationTypeStatus='Active',\n",
        "        RetryDelayInSeconds=315360000  # 10 years before retake possible\n",
        "\n",
        "    )\n",
        "    return response['QualificationType']['QualificationTypeId']\n",
        "\n",
        "\n",
        "# Create the qualification type with personalia_test_xml and answer_key_xml\n",
        "\n",
        "qualification_type_id_2 = create_qualification_type(\n",
        "    mturk_client=mturk,\n",
        "    name=\"NorwAI Norwegian Turk Personalia Qualification Preapproval v4\",\n",
        "    description=\"Denne kvalifikasjonsteksten er for å kartlegge våre arbeidere og vurdere deres kunnskaper i norsk og interesser\",\n",
        "    test=personalia_test_xml,\n",
        "    duration=1800  # Duration in seconds (30 minutes)\n",
        ")\n",
        "\n",
        "print(f\"Created Qualification Type ID: {qualification_type_id_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deployment and processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve all qualifications made"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_my_qualifications(mturk_client):\n",
        "    # Initialize an empty list to hold qualification types\n",
        "    my_qualifications = []\n",
        "\n",
        "    # Use pagination to retrieve all qualification types\n",
        "    next_token = None\n",
        "    while True:\n",
        "        if next_token:\n",
        "            response = mturk_client.list_qualification_types(\n",
        "                MustBeRequestable=True,\n",
        "                MustBeOwnedByCaller=True,\n",
        "                MaxResults=100,  # Adjust the number of results as needed\n",
        "                NextToken=next_token\n",
        "            )\n",
        "        else:\n",
        "            response = mturk_client.list_qualification_types(\n",
        "                MustBeRequestable=True,\n",
        "                MustBeOwnedByCaller=True,\n",
        "                MaxResults=100  # Adjust the number of results as needed\n",
        "            )\n",
        "\n",
        "        my_qualifications.extend(response['QualificationTypes'])\n",
        "\n",
        "        # Check if there are more qualifications to retrieve\n",
        "        next_token = response.get('NextToken')\n",
        "        if not next_token:\n",
        "            break\n",
        "\n",
        "    return my_qualifications\n",
        "\n",
        "\n",
        "qualifications = list_my_qualifications(mturk)\n",
        "for qual in qualifications:\n",
        "    print(f\"ID: {qual['QualificationTypeId']}, Name: {qual['Name']}, Description: {qual['Description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HIT deployment! running will deploy HITs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the qualification type ID for the personalia qualification used\n",
        "qualification_type_id_2 = \"3BEYYY5C1NI3YYW23CETY7BDKQQ17F\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_hit_with_xml_file(xml_file_path, mturk_client):\n",
        "    with open(xml_file_path, 'r') as file:\n",
        "        question_xml = file.read()\n",
        "    # Create HIT\n",
        "    response = mturk_client.create_hit(\n",
        "        Title='NorwAI Norwegian/Norsk Annotation',\n",
        "        Description='Les en nyhetsartikkel og gi et sammendrag og svar på spørsmål.',\n",
        "        Keywords='nyheter, annotering, sammendrag, lesing',\n",
        "        Reward='6.00',  # Adjust as necessary\n",
        "        MaxAssignments=2,\n",
        "        LifetimeInSeconds=1920000,  \n",
        "        AssignmentDurationInSeconds=1800,  # 30 minutes\n",
        "        Question=question_xml,\n",
        "        QualificationRequirements=[\n",
        "            {\n",
        "                'QualificationTypeId': qualification_type_id_2,\n",
        "                'Comparator': 'Exists',\n",
        "                'ActionsGuarded': 'Accept'\n",
        "            },\n",
        "            {'QualificationTypeId': \"000000000000000000L0\",\n",
        "                'Comparator': 'GreaterThan',\n",
        "                'IntegerValues': [60],\n",
        "                'ActionsGuarded': 'Accept'\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    return response['HIT']['HITId']\n",
        "\n",
        "output_directory = './HITS5'\n",
        "\n",
        "max_files_to_process = 148  # Set the number of files you want to process\n",
        "starting_point = 0  # Set the starting point for the files\n",
        "# Get the list of all xml files in the output directory\n",
        "all_xml_files = [f for f in sorted(os.listdir(output_directory)) if f.endswith('.xml')]\n",
        "# Process only a limited number of files based on max_files_to_process\n",
        "for xml_file in all_xml_files[starting_point:max_files_to_process]:\n",
        "    xml_file_path = os.path.join(output_directory, xml_file)\n",
        "    try:\n",
        "        hit_id = create_hit_with_xml_file(xml_file_path, mturk)\n",
        "        print(f\"Created HIT with ID: {hit_id}\")\n",
        "        # Adjust the preview link based on the endpoint\n",
        "        if 'sandbox' in endpoint_url:\n",
        "            preview_link = f\"https://workersandbox.mturk.com/mturk/preview?groupId={hit_id}\"\n",
        "        else:\n",
        "            preview_link = f\"https://worker.mturk.com/mturk/preview?groupId={hit_id}\"\n",
        "        print(f\"Preview your HIT: {preview_link}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while creating HIT for {xml_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approve qualifications manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This is needed for the workers personalia to be saved and for them to be able to actuall accept and take a HIT assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check that alder is not 25-34 and that Norsk språkferdigheter is Flytende\n",
        "def check_qualification(answers):\n",
        "    for answer in answers:\n",
        "#        if answer['Question ID'] == 'alder' and answer['Answer'] == '25-34':\n",
        "#            return False\n",
        "        if answer['Question ID'] == 'Norsk språkferdigheter' and answer['Answer'] != 'Flytende':\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Approves all pending workers for a qualification\n",
        "#### Warning!!! After approval all answers will not be retrievable again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "qualification_type_id = '3BEYYY5C1NI3YYW23CETY7BDKQQ17F'\n",
        "file_name = 'mturk_qualification_personalia_second_experiment.json'\n",
        "try:\n",
        "    # Retrieve the list of qualification requests\n",
        "    qualification_requests = mturk.list_qualification_requests(\n",
        "        QualificationTypeId=qualification_type_id,\n",
        "        MaxResults=100  # Adjust as needed, maximum is 100\n",
        "    )\n",
        "\n",
        "    # Iterate over the qualification requests\n",
        "    for request in qualification_requests.get('QualificationRequests', []):\n",
        "        request_id = request['QualificationRequestId']\n",
        "        if not check_qualification(parse_answer_xml(request[\"Answer\"])[0]):\n",
        "            print(f\"Qualification request {request_id} did not meet the criteria. Rejecting...\")\n",
        "            # Reject the qualification request\n",
        "            mturk.reject_qualification_request(\n",
        "                QualificationRequestId=request_id,\n",
        "                Reason='Your answers did not meet the criteria for this qualification.'\n",
        "            )\n",
        "            continue\n",
        "        # Convert the request to a JSON string\n",
        "        request_json = json.dumps(request, indent=4, default=str)\n",
        "\n",
        "        # Append the JSON string to the file\n",
        "        with open(file_name, 'a') as file:\n",
        "            file.write(request_json + '\\n')\n",
        "\n",
        "        # Approve each qualification request\n",
        "        mturk.accept_qualification_request(\n",
        "            QualificationRequestId=request_id\n",
        "        )\n",
        "        print(f\"Approved and logged request: {request_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(f\"All processed requests have been appended to {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval and postprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve all HITs assignments from Turk to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import datetime\n",
        "from datetime import timezone\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "\n",
        "# Cache directory and file setup\n",
        "cache_directory = 'mturk_cache'\n",
        "cache_file = 'hit_data.json'\n",
        "fetched_hits_file = os.path.join(cache_directory, 'fetched_hits.json')\n",
        "full_cache_path = os.path.join(cache_directory, cache_file)\n",
        "os.makedirs(cache_directory, exist_ok=True)\n",
        "\n",
        "def serialize_datetime(obj):\n",
        "    \"\"\" Helper function to serialize datetime objects for JSON. \"\"\"\n",
        "    if isinstance(obj, datetime.datetime):\n",
        "        return obj.isoformat()\n",
        "    raise TypeError(\"Type %s not serializable\" % type(obj))\n",
        "\n",
        "def preprocess_hit(hit):\n",
        "    \"\"\" Preprocess a HIT to ensure all data is serializable. \"\"\"\n",
        "    for key, value in hit.items():\n",
        "        if isinstance(value, datetime.datetime):\n",
        "            hit[key] = value.isoformat()  # Convert datetime to ISO format string\n",
        "    return hit\n",
        "\n",
        "\n",
        "def fetch_all_hits(mturk_client):\n",
        "    print(\"Fetching all HITs from MTurk...\")\n",
        "    all_hits = []\n",
        "    next_token = None\n",
        "    cutoff_date = datetime.datetime(2024, 5, 18, tzinfo=timezone.utc)  # Ensure cutoff_date is offset-aware\n",
        "    while True:\n",
        "        response = mturk_client.list_hits(NextToken=next_token) if next_token else mturk_client.list_hits()\n",
        "        processed_hits = [\n",
        "            preprocess_hit(hit) for hit in response['HITs']\n",
        "            if hit['CreationTime'].replace(tzinfo=timezone.utc) > cutoff_date  # Assume hit['CreationTime'] is datetime\n",
        "        ]\n",
        "        all_hits.extend(processed_hits)\n",
        "        next_token = response.get('NextToken')\n",
        "        if not next_token:\n",
        "            break\n",
        "    save_json(all_hits, fetched_hits_file)\n",
        "    print(\"All HITs fetched and saved.\")\n",
        "\n",
        "\n",
        "def serialize_datetime(obj):\n",
        "    \"\"\" Helper function to serialize datetime and timedelta objects for JSON. \"\"\"\n",
        "    if isinstance(obj, datetime.datetime):\n",
        "        return obj.isoformat()\n",
        "    elif isinstance(obj, datetime.timedelta):\n",
        "        # Convert timedelta to total seconds for serialization\n",
        "        return obj.total_seconds()\n",
        "    raise TypeError(\"Type %s not serializable\" % type(obj))\n",
        "\n",
        "def save_json(data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        # Pass the custom serializer to handle datetime and timedelta objects\n",
        "        json.dump(data, file, default=serialize_datetime, indent=4)\n",
        "\n",
        "\n",
        "def load_json(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        return {}\n",
        "\n",
        "def load_cache():\n",
        "    return load_json(full_cache_path)\n",
        "\n",
        "def save_cache(cache):\n",
        "    save_json(cache, full_cache_path)\n",
        "\n",
        "def convert_to_utc(dt_input):\n",
        "    \"\"\" Convert an ISO formatted string or a datetime object with timezone to UTC datetime object. \"\"\"\n",
        "    if isinstance(dt_input, str):\n",
        "        # Convert from string to datetime with timezone awareness\n",
        "        dt = datetime.datetime.fromisoformat(dt_input)\n",
        "    elif isinstance(dt_input, datetime.datetime):\n",
        "        # Use the datetime object directly if it's already a datetime\n",
        "        dt = dt_input\n",
        "    else:\n",
        "        # Handle cases where the input is neither a str nor datetime\n",
        "        raise TypeError(f\"Expected string or datetime for conversion, got {type(dt_input)}\")\n",
        "\n",
        "    # Convert to UTC if it's not already in UTC\n",
        "    if dt.tzinfo is not None:\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    else:\n",
        "        # Assume naive datetime objects are in UTC\n",
        "        return dt.replace(tzinfo=timezone.utc)\n",
        "\n",
        "\n",
        "def check_hit_and_fetch_assignments(mturk_client, hit, cache, current_time):\n",
        "    hit_id = hit['HITId']\n",
        "    expiration = convert_to_utc(hit['Expiration'])  # Safely convert without assuming the type\n",
        "    current_time = current_time.astimezone(timezone.utc)  # Ensure current_time is in UTC\n",
        "\n",
        "    cache_hit = cache.get(hit_id)\n",
        "    if cache_hit:\n",
        "        cache_expiration = convert_to_utc(cache_hit['Expiration'])\n",
        "        if cache_expiration < current_time or (cache_hit['NumberOfAssignmentsAvailable'] == 0 and cache_hit['NumberOfAssignmentsPending'] == 0):\n",
        "            return {'hit_id': hit_id, 'data': cache_hit['Data']}\n",
        "\n",
        "    assignments = mturk_client.list_assignments_for_hit(HITId=hit_id)['Assignments']\n",
        "    assignment_data = [{\n",
        "        'HITId': hit_id,\n",
        "        'AssignmentId': assn['AssignmentId'],\n",
        "        'AssignmentStatus': assn['AssignmentStatus'],\n",
        "        'AcceptTime': convert_to_utc(assn['AcceptTime']).isoformat(),\n",
        "        'SubmitTime': convert_to_utc(assn['SubmitTime']).isoformat(),\n",
        "        'Duration': convert_to_utc(assn['SubmitTime']) - convert_to_utc(assn['AcceptTime']),\n",
        "        'WorkerId': assn['WorkerId'],\n",
        "        'Answer': assn['Answer'],\n",
        "        'Question': mturk_client.get_hit(HITId=hit_id)[\"HIT\"][\"Question\"]\n",
        "    } for assn in assignments]\n",
        "\n",
        "    cache[hit_id] = {\n",
        "        'Expiration': expiration.isoformat(),\n",
        "        'NumberOfAssignmentsAvailable': hit['NumberOfAssignmentsAvailable'],\n",
        "        'NumberOfAssignmentsPending': hit['NumberOfAssignmentsPending'],\n",
        "        'Data': assignment_data\n",
        "    }\n",
        "    return {'hit_id': hit_id, 'data': assignment_data}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fetch_all_hits(mturk)  # Uncomment to fetch all HITs if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "from datetime import timezone\n",
        "\n",
        "def count_non_expired_assignments_in_cache():\n",
        "    cache = load_cache()  # Load the cache which contains HITs and their details\n",
        "    current_time = datetime.datetime.now(timezone.utc)\n",
        "    total_available = 0\n",
        "    total_pending = 0\n",
        "\n",
        "    # Iterate through each HIT in the cache\n",
        "    for hit_id, hit_data in cache.items():\n",
        "        # Check if the HIT has expired\n",
        "        expiration_time = datetime.datetime.fromisoformat(hit_data['Expiration'].replace('Z', '+00:00'))\n",
        "        if expiration_time > current_time:\n",
        "            total_available += hit_data.get('NumberOfAssignmentsAvailable', 0)\n",
        "            total_pending += hit_data.get('NumberOfAssignmentsPending', 0)\n",
        "\n",
        "    return total_available, total_pending\n",
        "\n",
        "# Usage\n",
        "total_available, total_pending = count_non_expired_assignments_in_cache()\n",
        "print(f\"Total Available Assignments for Non-Expired HITs: {total_available}\")\n",
        "print(f\"Total Pending Assignments for Non-Expired HITs: {total_pending}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hit_results(mturk_client):\n",
        "    cache = load_cache()\n",
        "    hits = load_json(fetched_hits_file)\n",
        "    current_time = datetime.datetime.now(timezone.utc)\n",
        "    all_results = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
        "        future_to_hit = {executor.submit(check_hit_and_fetch_assignments, mturk_client, hit, cache, current_time): hit for hit in hits}\n",
        "        for future in concurrent.futures.as_completed(future_to_hit):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                all_results.extend(result['data'])\n",
        "\n",
        "    save_cache(cache)\n",
        "    return pd.DataFrame(all_results)\n",
        "\n",
        "# Example Usage\n",
        "results_df = get_hit_results(mturk)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "def extract_questions_from_xml(xml_file):\n",
        "    # Assuming each XML file has a structure where the question text can be extracted\n",
        "    # Modify this function according to the actual structure of your XML files\n",
        "    with open(xml_file, 'r') as file:\n",
        "        question_xml = file.read()\n",
        "    return extract_texts_from_xml(question_xml)\n",
        "\n",
        "def extract_texts_from_xml(xml_q):\n",
        "    parsed_question = parse_question_xml(xml_q)\n",
        "    questions_123 = []\n",
        "    for question in parsed_question:\n",
        "        if \"Tekst\" in question[\"Title\"]:\n",
        "            questions_123.append(question[\"Text\"])\n",
        "    return questions_123\n",
        "\n",
        "# Paths to your folders\n",
        "folders = ['./HITS5']\n",
        "# Set to store questions from XML files\n",
        "xml_questions = {}\n",
        "# Read each XML file and extract questions\n",
        "for folder in folders:\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith('.xml'):\n",
        "            question = extract_questions_from_xml(os.path.join(folder, filename))\n",
        "            xml_questions[tuple(question)] = folder + \"/\" + filename\n",
        "\n",
        "# lets make a column in df that has the question text, and another column with the xml file\n",
        "results_df['QuestionText'] = results_df['Question'].apply(lambda x: tuple(extract_texts_from_xml(x)))\n",
        "results_df['QuestionXML'] = results_df['Question'].apply(lambda x: xml_questions[tuple(extract_texts_from_xml(x))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import pytz  # Or from zoneinfo import ZoneInfo if Python 3.9+\n",
        "\n",
        "def expire_all_active_hits(mturk_client):\n",
        "    # Get yesterday's UTC time\n",
        "    utc_now = datetime.now(pytz.UTC)  # Or datetime.now(ZoneInfo(\"UTC\"))\n",
        "    utc_yesterday = utc_now - timedelta(days=1)\n",
        "\n",
        "    # Retrieve all HITs\n",
        "    next_token = None\n",
        "    while True:\n",
        "        if next_token:\n",
        "            response = mturk_client.list_hits(NextToken=next_token)\n",
        "        else:\n",
        "            response = mturk_client.list_hits()\n",
        "\n",
        "        for hit in response['HITs']:\n",
        "            # Update the expiration time to yesterday\n",
        "            mturk_client.update_expiration_for_hit(HITId=hit['HITId'], ExpireAt=utc_yesterday)\n",
        "\n",
        "        next_token = response.get('NextToken')\n",
        "        if not next_token:\n",
        "            break\n",
        "\n",
        "# Example usag\n",
        "#expire_all_active_hits(mturk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check df content and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get date now\n",
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "# format date to year month day\n",
        "dt_string = now.strftime(\"%d-%m-%Y %H-%M-%S\")\n",
        "# make or check folder exists\n",
        "folder_name = f\"HIT_results\"\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "file_name = f\"contains_dataset_and_not_contains_hit_results_all_with_questions_{dt_string}.csv\"\n",
        "results_df.to_csv(os.path.join(folder_name, file_name), index=False)\n",
        "print(f\"Results exported to {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parsing results into readable formats and txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the most recent results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import os\n",
        "# Specify the folder name\n",
        "folder_name = \"HIT_results\"\n",
        "\n",
        "# Get the list of CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(folder_name) if f.endswith('.csv')]\n",
        "\n",
        "# Sort the CSV files based on the modification time\n",
        "csv_files.sort(key=lambda x: os.path.getmtime(os.path.join(folder_name, x)))\n",
        "\n",
        "# Select the newest CSV file\n",
        "newest_file = csv_files[-1]\n",
        "df = pd.read_csv(os.path.join(folder_name, newest_file))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Format the dataframe into readable txt files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = './ParsedNotFilteredHITS3'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save all with filtering >=2 correct multiple choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "# Main loop for file generation\n",
        "for index, row in df.iterrows():\n",
        "    question_text = parse_question_xml(row['Question'])\n",
        "    answer_text, passed = parse_answer_xml(row['Answer'])\n",
        "    if passed < 0:\n",
        "        print(f\"Worker {row['WorkerId']} did not pass all questions for HIT {row['HITId']}\")\n",
        "        print(passed)\n",
        "        continue\n",
        "\n",
        "    json_data = {\n",
        "        \"WorkerId\": row[\"WorkerId\"],\n",
        "        \"HITId\": row[\"HITId\"],\n",
        "        \"AssignmentId\": row[\"AssignmentId\"],\n",
        "        \"AssignmentStatus\": row.get(\"AssignmentStatus\", \"\"),\n",
        "        \"AcceptTime\": row.get(\"AcceptTime\", \"\"),\n",
        "        \"SubmitTime\": row.get(\"SubmitTime\", \"\"),\n",
        "        \"Duration\": row.get(\"Duration\", \"\"),\n",
        "        \"QuestionsAndAnswers\": []\n",
        "    }\n",
        "\n",
        "    text_content = \"\"\n",
        "    for key, value in json_data.items():\n",
        "        if key != \"QuestionsAndAnswers\":\n",
        "            text_content += f\"{key}: {value}\\n\"\n",
        "\n",
        "    # Grouping questions and answers\n",
        "    for q in question_text:\n",
        "        q_index = q['Title'].split()[-1]\n",
        "        related_answers = [a for a in answer_text if a['Question ID'].startswith(f\"text{q_index}_\")]\n",
        "        qa_pairs = {\n",
        "            \"Title\": q['Title'],\n",
        "            \"QuestionText\": q['Text'],\n",
        "            \"Answers\": related_answers\n",
        "        }\n",
        "        json_data[\"QuestionsAndAnswers\"].append(qa_pairs)\n",
        "\n",
        "        text_content += f\"\\nTitle: {q['Title']}\\nQuestion Text: {q['Text']}\\n\"\n",
        "        for a in related_answers:\n",
        "            text_content += f\"- {a['Question ID']}: {a['Answer']}\\n\"\n",
        "\n",
        "    # Writing JSON file\n",
        "    json_file_path = os.path.join(output_dir, f\"{row['AssignmentId']}.json\")\n",
        "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "    # Writing Text file\n",
        "    txt_file_path = os.path.join(output_dir, f\"{row['AssignmentId']}.txt\")\n",
        "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
        "        txt_file.write(text_content)\n",
        "\n",
        "print(\"Files saved in directory:\", output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Approve or reject HIT assignments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter based on correct multiple choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the userStory script\n",
        "from HITOrganizer import HITOrganizer\n",
        "from BanFilter import BanFilter\n",
        "import sys\n",
        "from AssignmentFilter import AssignmentFilter\n",
        "\n",
        "# Organiserer HITs og brukerprofiler, hvis problem med koden skyldes path\n",
        "hit_organizer = HITOrganizer(\"ParsedNotFilteredHITS3\", \"UserProfiles3\")\n",
        "\n",
        "assignment_filter = AssignmentFilter('ParsedNotFilteredHITS3') # Filterer ut assignments som ikke har bestått, hvis det er problem med koden skyldes path\n",
        "rejected_assignment_ids = assignment_filter.filter_assignments()\n",
        "\n",
        "workers_to_ban = BanFilter('ParsedNotFilteredHITS3').get_worker_ids()\n",
        "print(len(workers_to_ban))\n",
        "print(len(rejected_assignment_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get approved hits by filtering out rejected_assignment_ids from df, and filtering to where df is Submitted, then to a list\n",
        "approved_hits = df[~df['AssignmentId'].isin(rejected_assignment_ids)]\n",
        "approved_hits = approved_hits[approved_hits['AssignmentStatus'] == 'Submitted']\n",
        "approved_hits_list = approved_hits['AssignmentId'].tolist()\n",
        "len(approved_hits_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter out workers that have been rejected already in df from rejected_assignment_ids, where df AssignmentStatus is not Rejected\n",
        "#rejected_assignment_ids = [a for a in rejected_assignment_ids if a in df[df['AssignmentStatus'] == 'Submitted']['AssignmentId'].values]\n",
        "#rejected_assignment_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Can be used by inverting the filtering condition above, and then rejecting the assignments\n",
        "def reject_hit(assignment_id):\n",
        "    try:\n",
        "        response = mturk.reject_assignment(\n",
        "            AssignmentId=assignment_id,\n",
        "            RequesterFeedback='Your work did not meet the required standards, as you had too few correct multiple choice answers or wrong sourcing. We encurage you to try again!',\n",
        "            # You can customize the RequesterFeedback message as needed\n",
        "        )\n",
        "        hit_organizer.organize_file(assignment_id, approve=False)\n",
        "        print(f\"Rejected HIT: {assignment_id}\")\n",
        "    except mturk.exceptions.RequestError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# iterate passed_assignment_ids\n",
        "for assignment_id in rejected_assignment_ids:\n",
        "    reject_hit(assignment_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def approve_hit(assignment_id):\n",
        "    try:\n",
        "        response = mturk.approve_assignment(\n",
        "            AssignmentId=assignment_id,\n",
        "            RequesterFeedback='Good work, thank you!',\n",
        "            OverrideRejection=False\n",
        "        )\n",
        "        \n",
        "        hit_organizer.organize_file(assignment_id, approve=True)\n",
        "        print(f\"Approved HIT: {assignment_id}\")\n",
        "    except mturk.exceptions.RequestError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# iterate passed_assignment_ids\n",
        "#for assignment_id in approved_hits:\n",
        "#    approve_hit(assignment_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ban_worker(worker_id):\n",
        "    try:\n",
        "        response = mturk.create_worker_block(\n",
        "            WorkerId=worker_id,\n",
        "            Reason='Repeatedly submitting low-quality work',\n",
        "            # You can customize the Reason message as needed\n",
        "        )\n",
        "        print(f\"Blocked Worker: {worker_id}\")\n",
        "    except mturk.exceptions.RequestError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "for worker in workers_to_ban:\n",
        "   ban_worker(worker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redeploy missing assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "def retrieve_and_count_questions(mturk_client):\n",
        "    questions_data = {}  # Stores data for each question\n",
        "    # Retrieve all HITs\n",
        "    next_token = None\n",
        "    while True:\n",
        "        response = mturk_client.list_hits(NextToken=next_token) if next_token else mturk_client.list_hits()\n",
        "        for hit in response['HITs']:\n",
        "            hit_id = hit['HITId']\n",
        "            question = hit['Question']  # Assuming direct access to question text\n",
        "            is_expired = hit['Expiration'] < datetime.now(timezone.utc)\n",
        "            parsed_question = tuple(extract_texts_from_xml(question))\n",
        "            if parsed_question not in questions_data:\n",
        "                questions_data[parsed_question] = {'total_count': 0, 'hit_ids': set(), 'question_xml': hit['Question']}\n",
        "\n",
        "            assignments_response = mturk_client.list_assignments_for_hit(HITId=hit_id)\n",
        "            for assignment in assignments_response['Assignments']:\n",
        "                if assignment['AssignmentStatus'] in ['Submitted', 'Approved', 'Pending'] and assignment['AssignmentId'] not in rejected_assignment_ids:\n",
        "                    questions_data[parsed_question]['total_count'] += 1\n",
        "            \n",
        "            questions_data[parsed_question]['total_count'] += hit['NumberOfAssignmentsPending']\n",
        "            \n",
        "            if not is_expired:\n",
        "                questions_data[parsed_question]['total_count'] += hit['NumberOfAssignmentsAvailable']\n",
        "\n",
        "            questions_data[parsed_question]['hit_ids'].add(hit_id)\n",
        "\n",
        "        next_token = response.get('NextToken')\n",
        "        if not next_token:\n",
        "            break\n",
        "\n",
        "    return questions_data\n",
        "\n",
        "questions_data = retrieve_and_count_questions(mturk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative quicker method, but not valid under all conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import datetime\n",
        "from datetime import timezone\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "# Cache directory and file setup\n",
        "cache_directory = 'mturk_cache'\n",
        "fetched_hits_file = os.path.join(cache_directory, 'fetched_hits.json')\n",
        "\n",
        "def load_cached_hits():\n",
        "    with open(fetched_hits_file, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def process_hit_and_count_questions(hit, mturk_client, questions_data, lock):\n",
        "    hit_id = hit['HITId']\n",
        "    question = hit['Question']\n",
        "    expiration = datetime.datetime.fromisoformat(hit['Expiration']).replace(tzinfo=timezone.utc)\n",
        "    is_expired = expiration < datetime.datetime.now(timezone.utc)\n",
        "    parsed_question = tuple(extract_texts_from_xml(question))\n",
        "\n",
        "    assignments_response = mturk_client.list_assignments_for_hit(HITId=hit_id)\n",
        "    count = 0  # Local count to minimize lock duration\n",
        "    for assignment in assignments_response['Assignments']:\n",
        "        if assignment['AssignmentStatus'] in ['Submitted', 'Approved'] and assignment['AssignmentId'] not in rejected_assignment_ids:\n",
        "            count += 1\n",
        "\n",
        "    with lock:\n",
        "        if parsed_question not in questions_data:\n",
        "            questions_data[parsed_question] = {'total_count': 0, 'hit_ids': set(), 'question_xml': question}\n",
        "\n",
        "        # Add the count of valid assignments to total_count\n",
        "        questions_data[parsed_question]['total_count'] += count + hit['NumberOfAssignmentsPending']\n",
        "        if not is_expired:\n",
        "            questions_data[parsed_question]['total_count'] += hit['NumberOfAssignmentsAvailable']\n",
        "        questions_data[parsed_question]['hit_ids'].add(hit_id)\n",
        "\n",
        "def retrieve_and_count_questions_from_cache(mturk_client):\n",
        "    questions_data = {}\n",
        "    all_hits = load_cached_hits()  # Load cached HITs from a JSON file\n",
        "    lock = threading.Lock()  # Lock for thread-safe operations on questions_data\n",
        "\n",
        "    # Use ThreadPoolExecutor to parallelize processing of hits\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:\n",
        "        futures = [executor.submit(process_hit_and_count_questions, hit, mturk_client, questions_data, lock) for hit in all_hits]\n",
        "        concurrent.futures.wait(futures)\n",
        "\n",
        "    return questions_data\n",
        "\n",
        "# Example usage\n",
        "# Assuming mturk_client is set up and rejected_assignment_ids is populated\n",
        "questions_data = retrieve_and_count_questions_from_cache(mturk)\n",
        "questions_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Redeploy missing assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast  # Import the ast module for safely evaluating strings as Python expressions\n",
        "# Iterate through each question in questions_data\n",
        "for question_tuple in questions_data:\n",
        "    # Convert the tuple to a string format for comparison\n",
        "    question_string = str(question_tuple) \n",
        "    # Find matching QuestionXML in df for the current question_string\n",
        "    matching_xml = df[df['QuestionText'] == question_string]['QuestionXML']\n",
        "    if not matching_xml.empty:\n",
        "        # Since the same QuestionText always has the same QuestionXML, take the first match\n",
        "        questions_data[question_tuple]['xml_file'] = matching_xml.iloc[0]\n",
        "    else:\n",
        "        # Handle cases where the question text does not exist in the DataFrame\n",
        "        print(f\"XML data not found for question tuple: {question_tuple}\")\n",
        "# Get a sample from questions_data\n",
        "sample_question_data = questions_data[next(iter(questions_data))]\n",
        "print(sample_question_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = 0\n",
        "additional_assignments = 0\n",
        "for question, data in questions_data.items():\n",
        "        if data['total_count'] < 2:\n",
        "            additional_assignments += 2 - data['total_count']\n",
        "            counter += 1\n",
        "\n",
        "print(f\"Number of questions with less than 3 assignments: {counter}\")\n",
        "print(f\"Total additional assignments needed: {additional_assignments}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_consolidated_additional_hits(mturk_client, questions_data, qualification_type_id):\n",
        "    counter = 0\n",
        "    ass_counter = 0\n",
        "    for question, data in questions_data.items():\n",
        "        try:\n",
        "            # read xml_file from questions data to get the question_xml\n",
        "            with open(data[\"xml_file\"], 'r') as file:\n",
        "                question_xml = file.read()\n",
        "        except FileNotFoundError:\n",
        "            print(\"File not found.\")\n",
        "            continue\n",
        "        if data['total_count'] < 2:\n",
        "            additional_assignments = 2 - data['total_count']\n",
        "            counter += 1\n",
        "            ass_counter += additional_assignments\n",
        "            new_hit_id = mturk_client.create_hit(\n",
        "                Title='NorwAI Norwegian/Norsk Annotation',\n",
        "                Description='Les en nyhetsartikkel og gi et sammendrag og svar på spørsmål.',\n",
        "                Keywords='nyheter, annotering, sammendrag, lesing',\n",
        "                Reward='6.00',\n",
        "                MaxAssignments=additional_assignments,\n",
        "                LifetimeInSeconds=1920000,\n",
        "                AssignmentDurationInSeconds=1800,\n",
        "                Question=question_xml,\n",
        "                QualificationRequirements=[{\n",
        "                    'QualificationTypeId': qualification_type_id,\n",
        "                    'Comparator': 'Exists',\n",
        "                    'ActionsGuarded': 'Accept'\n",
        "                },\n",
        "                {'QualificationTypeId': \"000000000000000000L0\",\n",
        "                     'Comparator': 'GreaterThan',\n",
        "                     'IntegerValues': [60],\n",
        "                     'ActionsGuarded': 'Accept'}\n",
        "                ],\n",
        "            )['HIT']['HITId']\n",
        "            print(f\"Created new HIT with ID: {new_hit_id} for question '{data['xml_file']}' with {additional_assignments} additional assignments.\")\n",
        "    return counter, ass_counter\n",
        "\n",
        "qualification_type_id_2 = \"3BEYYY5C1NI3YYW23CETY7BDKQQ17F\" # new personalia we filter on\n",
        "create_consolidated_additional_hits(mturk, questions_data, qualification_type_id_2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
